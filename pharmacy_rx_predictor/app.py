"""
è–¬å±€ å¹´é–“å‡¦æ–¹ç®‹æšæ•° äºˆæ¸¬ãƒ„ãƒ¼ãƒ« v2.0
====================================
åšç”ŸåŠ´åƒçœã€Œè–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦ã€ãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆã™ã‚‹ã‚¢ãƒ—ãƒªã€‚

ã€v2.0 ä¿®æ­£ç‚¹ã€‘
- æ­£ã—ã„APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ä¿®æ­£ï¼ˆSpring Boot ãƒ™ãƒ¼ã‚¹ï¼‰
- è–¬å±€åã®ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼ˆå€™è£œä¸€è¦§ã‹ã‚‰ã®é¸æŠï¼‰
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®ä¿®æ­£
- å‡¦æ–¹ç®‹ãƒ‡ãƒ¼ã‚¿è§£æã®å¼·åŒ–
"""

import re
import time
import urllib.parse
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

import requests
import streamlit as st
from bs4 import BeautifulSoup

# ---------------------------------------------------------------------------
# å®šæ•°ãƒ»å‚ç…§ãƒ‡ãƒ¼ã‚¿
# ---------------------------------------------------------------------------

PREFECTURES = [
    "åŒ—æµ·é“", "é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "å±±å½¢çœŒ", "ç¦å³¶çœŒ",
    "èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ",
    "æ–°æ½ŸçœŒ", "å¯Œå±±çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", "é•·é‡çœŒ",
    "å²é˜œçœŒ", "é™å²¡çœŒ", "æ„›çŸ¥çœŒ", "ä¸‰é‡çœŒ",
    "æ»‹è³€çœŒ", "äº¬éƒ½åºœ", "å¤§é˜ªåºœ", "å…µåº«çœŒ", "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ",
    "é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å²¡å±±çœŒ", "åºƒå³¶çœŒ", "å±±å£çœŒ",
    "å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ",
    "ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ", "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", "æ²–ç¸„çœŒ",
]

PREFECTURE_CODES: Dict[str, str] = {
    "åŒ—æµ·é“": "01", "é’æ£®çœŒ": "02", "å²©æ‰‹çœŒ": "03", "å®®åŸçœŒ": "04", "ç§‹ç”°çœŒ": "05",
    "å±±å½¢çœŒ": "06", "ç¦å³¶çœŒ": "07", "èŒ¨åŸçœŒ": "08", "æ ƒæœ¨çœŒ": "09", "ç¾¤é¦¬çœŒ": "10",
    "åŸ¼ç‰çœŒ": "11", "åƒè‘‰çœŒ": "12", "æ±äº¬éƒ½": "13", "ç¥å¥ˆå·çœŒ": "14", "æ–°æ½ŸçœŒ": "15",
    "å¯Œå±±çœŒ": "16", "çŸ³å·çœŒ": "17", "ç¦äº•çœŒ": "18", "å±±æ¢¨çœŒ": "19", "é•·é‡çœŒ": "20",
    "å²é˜œçœŒ": "21", "é™å²¡çœŒ": "22", "æ„›çŸ¥çœŒ": "23", "ä¸‰é‡çœŒ": "24", "æ»‹è³€çœŒ": "25",
    "äº¬éƒ½åºœ": "26", "å¤§é˜ªåºœ": "27", "å…µåº«çœŒ": "28", "å¥ˆè‰¯çœŒ": "29", "å’Œæ­Œå±±çœŒ": "30",
    "é³¥å–çœŒ": "31", "å³¶æ ¹çœŒ": "32", "å²¡å±±çœŒ": "33", "åºƒå³¶çœŒ": "34", "å±±å£çœŒ": "35",
    "å¾³å³¶çœŒ": "36", "é¦™å·çœŒ": "37", "æ„›åª›çœŒ": "38", "é«˜çŸ¥çœŒ": "39", "ç¦å²¡çœŒ": "40",
    "ä½è³€çœŒ": "41", "é•·å´çœŒ": "42", "ç†Šæœ¬çœŒ": "43", "å¤§åˆ†çœŒ": "44", "å®®å´çœŒ": "45",
    "é¹¿å…å³¶çœŒ": "46", "æ²–ç¸„çœŒ": "47",
}

# å…¨å›½çµ±è¨ˆï¼ˆåšç”ŸåŠ´åƒçœã€Œèª¿å‰¤åŒ»ç™‚è²»ã®å‹•å‘ã€2022å¹´åº¦ï¼‰
NATIONAL_STATS = {
    "fiscal_year": 2022,
    "total_prescriptions": 885_000_000,
    "total_pharmacies": 61_860,
    "average_per_year": 14_305,
    "median_estimate": 8_000,
    "daily_average": 44,
    "working_days": 305,
    "source": "åšç”ŸåŠ´åƒçœã€Œèª¿å‰¤åŒ»ç™‚è²»ï¼ˆé›»ç®—å‡¦ç†åˆ†ï¼‰ã®å‹•å‘ã€ï¼ˆ2022å¹´åº¦ï¼‰ãƒ»æ—¥æœ¬è–¬å‰¤å¸«ä¼šèª¿æŸ»",
    "source_url": "https://www.mhlw.go.jp/topics/medias/med/",
}

# å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³è–¬å±€ãƒ‡ãƒ¼ã‚¿ï¼ˆå„ç¤¾IRãƒ»è–¬å±€æ©Ÿèƒ½æƒ…å ±ã‚ˆã‚Šæ¨è¨ˆï¼‰
CHAIN_DATA: Dict[str, Dict] = {
    "ã‚¦ã‚¨ãƒ«ã‚·ã‚¢":        {"annual_est": 45_000, "min": 20_000, "max": 80_000,  "ir": "ã‚¦ã‚¨ãƒ«ã‚·ã‚¢HD çµ±åˆå ±å‘Šæ›¸"},
    "ãƒ„ãƒ«ãƒ":            {"annual_est": 30_000, "min": 15_000, "max": 60_000,  "ir": "ãƒ„ãƒ«ãƒHD æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸"},
    "ãƒãƒ„ãƒ¢ãƒˆã‚­ãƒ¨ã‚·":    {"annual_est": 18_000, "min": 8_000,  "max": 35_000,  "ir": "ãƒãƒ„ã‚­ãƒ¨ã‚³ã‚¹ãƒ¢ã‚¹ IRè³‡æ–™"},
    "ãƒãƒ„ã‚­ãƒ¨":          {"annual_est": 18_000, "min": 8_000,  "max": 35_000,  "ir": "ãƒãƒ„ã‚­ãƒ¨ã‚³ã‚¹ãƒ¢ã‚¹ IRè³‡æ–™"},
    "ã‚³ã‚¹ãƒ¢ã‚¹è–¬å“":      {"annual_est": 22_000, "min": 10_000, "max": 40_000,  "ir": "ã‚³ã‚¹ãƒ¢ã‚¹è–¬å“ æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸"},
    "ã‚¹ã‚®è–¬å±€":          {"annual_est": 40_000, "min": 20_000, "max": 70_000,  "ir": "ã‚¹ã‚®HD IRè³‡æ–™"},
    "ã‚«ãƒ¯ãƒè–¬å“":        {"annual_est": 35_000, "min": 15_000, "max": 60_000,  "ir": "ã‚«ãƒ¯ãƒè–¬å“ æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸"},
    "ã‚¯ãƒªã‚¨ã‚¤ãƒˆ":        {"annual_est": 25_000, "min": 12_000, "max": 45_000,  "ir": "ã‚¯ãƒªã‚¨ã‚¤ãƒˆSDHD IR"},
    "ã‚µãƒ³ãƒ‰ãƒ©ãƒƒã‚°":      {"annual_est": 16_000, "min": 8_000,  "max": 30_000,  "ir": "ã‚µãƒ³ãƒ‰ãƒ©ãƒƒã‚° æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸"},
    "æ—¥æœ¬èª¿å‰¤":          {"annual_est": 55_000, "min": 25_000, "max": 120_000, "ir": "æ—¥æœ¬èª¿å‰¤ çµ±åˆå ±å‘Šæ›¸"},
    "ã‚¯ã‚ªãƒ¼ãƒ«":          {"annual_est": 35_000, "min": 15_000, "max": 70_000,  "ir": "ã‚¯ã‚ªãƒ¼ãƒ«HD IRè³‡æ–™"},
    "ã‚¢ã‚¤ãƒ³":            {"annual_est": 50_000, "min": 25_000, "max": 100_000, "ir": "ã‚¢ã‚¤ãƒ³HD æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸"},
    "ã‚¢ã‚¤ãƒ³ãƒ•ã‚¡ãƒ¼ãƒã‚·ãƒ¼ã‚º":{"annual_est": 50_000,"min": 25_000, "max": 100_000,"ir": "ã‚¢ã‚¤ãƒ³HD æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸"},
    "ãƒ•ã‚¡ãƒ¼ãƒãƒ©ã‚¤ã‚º":    {"annual_est": 30_000, "min": 15_000, "max": 55_000,  "ir": "ãƒ•ã‚¡ãƒ¼ãƒãƒ©ã‚¤ã‚ºHD IR"},
    "ç·åˆãƒ¡ãƒ‡ã‚£ã‚«ãƒ«":    {"annual_est": 40_000, "min": 20_000, "max": 80_000,  "ir": "ç·åˆãƒ¡ãƒ‡ã‚£ã‚«ãƒ« IRè³‡æ–™"},
    "ãã™ã‚Šã®ç¦å¤ªéƒ":    {"annual_est": 20_000, "min": 8_000,  "max": 40_000,  "ir": "IRãƒ»è–¬å±€æ©Ÿèƒ½æƒ…å ±ã‚ˆã‚Šæ¨è¨ˆ"},
    "ã‚»ã‚¤ãƒ ã‚¹":          {"annual_est": 15_000, "min": 6_000,  "max": 28_000,  "ir": "å¯Œå£«è–¬å“ã‚°ãƒ«ãƒ¼ãƒ— IR"},
    "ãƒ•ã‚¡ãƒ¼ãƒãƒƒã‚¯ã‚¹":    {"annual_est": 28_000, "min": 12_000, "max": 55_000,  "ir": "è–¬å±€æ©Ÿèƒ½æƒ…å ±é›†è¨ˆã‚ˆã‚Šæ¨è¨ˆ"},
}

# ---------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ---------------------------------------------------------------------------

@dataclass
class PharmacyCandidate:
    name: str
    address: str
    href: str
    pref_cd: str = ""
    kikan_cd: str = ""

@dataclass
class SearchResult:
    pharmacy_name: str
    annual_prescriptions: Optional[int] = None
    prescriptions_range: Optional[Tuple[int, int]] = None
    daily_estimate: Optional[int] = None
    data_source: str = "unknown"
    source_label: str = ""
    source_url: str = ""
    confidence: str = "low"
    pharmacy_type: str = ""
    methodology: List[str] = field(default_factory=list)
    references: List[Dict] = field(default_factory=list)
    mhlw_found: bool = False
    mhlw_has_rx_data: bool = False
    web_search_found: bool = False
    search_log: List[str] = field(default_factory=list)
    mhlw_fields: Dict[str, str] = field(default_factory=dict)


# ---------------------------------------------------------------------------
# 1. åšç”ŸåŠ´åƒçœ è–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰
# ---------------------------------------------------------------------------

class MHLWScraper:
    """
    åšç”ŸåŠ´åƒçœã€ŒåŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆï¼ˆãƒŠãƒ“ã‚¤ï¼‰ã€ãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰è–¬å±€ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã€‚
    ãƒãƒ¼ã‚¿ãƒ«: https://www.iryou.teikyouseido.mhlw.go.jp/znk-web/

    æ­£ã—ã„APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆèª¿æŸ»æ¸ˆã¿ï¼‰:
    - ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–: GET /znk-web/juminkanja/S2300/initialize
    - æ¤œç´¢ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: GET /znk-web/juminkanja/S2300/yakkyokuSearch?yakkyokuKeyword=XXX&searchJudgeKbn=2
    - æ¤œç´¢çµæœä¸€è¦§: GET /znk-web/juminkanja/S2400/initialize/{keyword}/?sjk=2&page=0&size=20&sortNo=1
    - è–¬å±€è©³ç´°: GET /znk-web/juminkanja/S2430/initialize?prefCd=XX&kikanCd=XXXXX&kikanKbn=5
    """

    DOMAIN = "https://www.iryou.teikyouseido.mhlw.go.jp"
    BASE   = DOMAIN + "/znk-web"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/121.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Referer": "https://www.iryou.teikyouseido.mhlw.go.jp/znk-web/juminkanja/S2300/initialize",
        })
        self._initialized = False
        self.csrf_token = ""

    def initialize_session(self) -> Tuple[bool, str]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒƒã‚­ãƒ¼ã¨CSRFãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        try:
            r = self.session.get(
                f"{self.BASE}/juminkanja/S2300/initialize",
                timeout=15,
                allow_redirects=True,
            )
            if r.status_code != 200:
                return False, f"åˆæœŸåŒ–å¤±æ•— HTTP {r.status_code}"

            soup = BeautifulSoup(r.text, "html.parser")
            csrf_meta = soup.find("meta", {"name": "_csrf"})
            if csrf_meta:
                self.csrf_token = csrf_meta.get("content", "")

            self._initialized = True
            return True, "OK"

        except requests.Timeout:
            return False, "æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ15ç§’ï¼‰"
        except requests.ConnectionError as e:
            return False, f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}"
        except Exception as e:
            return False, f"ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}"

    def search_candidates(
        self, keyword: str, pref_code: str = "", max_pages: int = 3
    ) -> Tuple[List[PharmacyCandidate], int, str]:
        """
        è–¬å±€åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã€å€™è£œãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚

        Returns:
            (candidates_list, total_count, status_message)
        """
        if not self._initialized:
            ok, msg = self.initialize_session()
            if not ok:
                return [], 0, f"ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–å¤±æ•—: {msg}"

        log_msgs = []

        try:
            # Step 1: æ¤œç´¢ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            params = {
                "yakkyokuKeyword": keyword,
                "yakkyokuKeyword2": "",
                "searchJudgeKbn": "2",
            }
            r = self.session.get(
                f"{self.BASE}/juminkanja/S2300/yakkyokuSearch",
                params=params,
                headers={"ajaxFlag": "true"},
                timeout=15,
            )

            if r.status_code != 200:
                return [], 0, f"æ¤œç´¢ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— HTTP {r.status_code}"

            try:
                j = r.json()
                if j.get("code") != "0":
                    return [], 0, f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {j.get('messages', 'ä¸æ˜')}"
            except Exception:
                return [], 0, "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®JSONè§£æå¤±æ•—"

            log_msgs.append(f"æ¤œç´¢ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ OK (keyword='{keyword}')")

            # Step 2: çµæœä¸€è¦§ã‚’å–å¾—ï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰
            all_candidates: List[PharmacyCandidate] = []
            total_count = 0
            encoded_keyword = urllib.parse.quote(keyword)

            for page in range(max_pages):
                params_list = {
                    "sjk": "2",
                    "page": str(page),
                    "size": "20",
                    "sortNo": "1",
                }
                if pref_code:
                    params_list["prefCd"] = pref_code

                r2 = self.session.get(
                    f"{self.BASE}/juminkanja/S2400/initialize/{encoded_keyword}/",
                    params=params_list,
                    timeout=15,
                )

                if r2.status_code != 200:
                    log_msgs.append(f"  page {page}: HTTP {r2.status_code} â†’ ä¸­æ–­")
                    break

                candidates, total = self._parse_candidate_list(r2.text)
                if page == 0:
                    total_count = total
                    log_msgs.append(f"  åˆè¨ˆ {total}ä»¶ãƒ’ãƒƒãƒˆ")

                all_candidates.extend(candidates)
                log_msgs.append(f"  page {page}: {len(candidates)}ä»¶å–å¾—ï¼ˆç´¯è¨ˆ {len(all_candidates)}ä»¶ï¼‰")

                if len(candidates) == 0 or len(all_candidates) >= total_count:
                    break

                time.sleep(0.3)

            status = f"{len(all_candidates)}ä»¶å–å¾— (å…¨{total_count}ä»¶ä¸­)"
            return all_candidates, total_count, status

        except requests.Timeout:
            return [], 0, "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"
        except Exception as e:
            return [], 0, f"ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}"

    def _parse_candidate_list(
        self, html: str
    ) -> Tuple[List[PharmacyCandidate], int]:
        """æ¤œç´¢çµæœä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ãƒ‘ãƒ¼ã‚¹"""
        soup = BeautifulSoup(html, "html.parser")
        candidates = []

        # åˆè¨ˆä»¶æ•°ã‚’å–å¾—
        total = 0
        page_text = soup.get_text()
        cnt_match = re.search(r"(\d{1,6})\s*ä»¶", page_text)
        if cnt_match:
            total = int(cnt_match.group(1))

        # å„è–¬å±€ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹
        items = soup.find_all("div", class_="item")

        # div.item ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã¿ã‚‹
        if not items:
            # ãƒªã‚¹ãƒˆè¦ç´ ã¨ã—ã¦ã®è–¬å±€åãƒªãƒ³ã‚¯ã‚’ç›´æ¥æ¢ã™
            items = soup.find_all(
                lambda tag: tag.name in ["li", "div", "tr"]
                and tag.find("a", href=re.compile(r"S2430"))
            )

        for item in items:
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã®éè–¬å±€itemã‚’é™¤å¤–: h3.name ã‚’æŒã¤ã‚‚ã®ã ã‘
            h3_name = item.find("h3", class_="name")
            if not h3_name:
                continue
            link = h3_name.find("a", href=True)
            if not link:
                continue

            name = link.get_text(strip=True)
            href = link.get("href", "")
            if not href:
                continue

            # çµ¶å¯¾URLã«å¤‰æ›
            if href.startswith("/"):
                href = self.DOMAIN + href
            elif not href.startswith("http"):
                href = self.DOMAIN + "/znk-web/" + href

            # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ prefCd / kikanCd ã‚’å–å¾—
            parsed = urllib.parse.urlparse(href)
            qp = dict(urllib.parse.parse_qsl(parsed.query))
            pref_cd = qp.get("prefCd", "")
            kikan_cd = qp.get("kikanCd", "")

            # ä½æ‰€ã‚’å–å¾—
            # MHLWãƒãƒ¼ã‚¿ãƒ«ã§ã¯ dt å†…ã« <img alt="ä½æ‰€"> ãŒã‚ã‚‹æ§‹é€ 
            address = ""
            for dl in item.find_all("dl"):
                dt = dl.find("dt")
                if not dt:
                    continue
                img = dt.find("img")
                dt_text = dt.get_text(strip=True)
                is_address = (img and "ä½æ‰€" in img.get("alt", "")) or any(
                    kw in dt_text for kw in ["ä½æ‰€", "æ‰€åœ¨åœ°"]
                )
                if is_address:
                    dd = dl.find("dd")
                    if dd:
                        # Googleãƒãƒƒãƒ—ãƒªãƒ³ã‚¯ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                        for a in dd.find_all("a"):
                            a.decompose()
                        raw_addr = dd.get_text(strip=True)
                        # ã€’XXX-XXXX ã‚’é™¤ã„ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªä½æ‰€ã‚’æŠ½å‡º
                        cleaned = re.sub(r"ã€’\s*\d{3}[-ï¼]\d{4}\s*", "", raw_addr)
                        cleaned = re.sub(r"\s+", " ", cleaned).strip()
                        address = cleaned[:60]
                        break

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã€’XXXXXå½¢å¼ã‹ã‚‰ä½æ‰€ã‚’æŠ½å‡º
            if not address:
                full_item_text = item.get_text(separator=" ", strip=True)
                addr_match = re.search(r"ã€’\s*\d{3}-?\d{4}\s+(\S+.{5,50}?)(?:\s+\(|\s+TEL|\s+é›»è©±|$)", full_item_text)
                if addr_match:
                    address = addr_match.group(1).strip()[:60]

            if name:
                candidates.append(
                    PharmacyCandidate(
                        name=name,
                        address=address,
                        href=href,
                        pref_cd=pref_cd,
                        kikan_cd=kikan_cd,
                    )
                )

        return candidates, max(total, len(candidates))

    def get_pharmacy_detail(
        self, candidate: PharmacyCandidate
    ) -> Tuple[Optional[Dict], str]:
        """è–¬å±€è©³ç´°ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦å‡¦æ–¹ç®‹å—ä»˜å›æ•°ç­‰ã‚’æŠ½å‡º"""
        if not self._initialized:
            ok, msg = self.initialize_session()
            if not ok:
                return None, f"ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–å¤±æ•—: {msg}"

        # prefCd ã¨ kikanCd ãŒåˆ†ã‹ã£ã¦ã„ã‚Œã°ç›´æ¥APIã‚’å©ã
        if candidate.pref_cd and candidate.kikan_cd:
            url = (
                f"{self.BASE}/juminkanja/S2430/initialize"
                f"?prefCd={candidate.pref_cd}"
                f"&kikanCd={candidate.kikan_cd}"
                f"&kikanKbn=5"
            )
        else:
            url = candidate.href
            if not url.startswith("http"):
                url = self.DOMAIN + url

        try:
            r = self.session.get(url, timeout=15)
            if r.status_code != 200:
                return None, f"è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•— HTTP {r.status_code}"

            data = self._parse_detail_page(r.text)
            data["source_url"] = url
            return data, "OK"

        except requests.Timeout:
            return None, "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"
        except Exception as e:
            return None, f"ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}"

    def _parse_detail_page(self, html: str) -> Dict:
        """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å‡¦æ–¹ç®‹å—ä»˜å›æ•°ç­‰ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, "html.parser")
        data: Dict = {}
        full_text = soup.get_text(separator=" ")

        # â”€â”€ æ–½è¨­å
        for tag in ["h1", "h2", "h3"]:
            el = soup.find(tag)
            if el:
                data["facility_name"] = el.get_text(strip=True)
                break

        # â”€â”€ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆth/td ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã‚’å…¨ã¦å–å¾—
        fields: Dict[str, str] = {}
        for row in soup.find_all("tr"):
            cells = row.find_all(["th", "td"])
            if len(cells) >= 2:
                key = cells[0].get_text(strip=True)
                val = cells[1].get_text(strip=True)
                if key:
                    fields[key] = val

        # dl/dt/dd å½¢å¼ã‚‚å¯¾å¿œ
        for dl in soup.find_all("dl"):
            dts = dl.find_all("dt")
            dds = dl.find_all("dd")
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True)
                val = dd.get_text(strip=True)
                if key:
                    fields[key] = val

        data["all_fields"] = fields

        # â”€â”€ å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã‚’æ¢ã™
        # è–¬å±€æ©Ÿèƒ½æƒ…å ±ã§ã¯ã€Œç·å–æ‰±å‡¦æ–¹ç®‹æ•°ã€= å ±å‘ŠæœŸæ—¥ã®å‰å¹´1å¹´é–“ã®å–æ‰±å‡¦æ–¹ç®‹æšæ•°
        rx_annual = None
        rx_period = None
        rx_raw = None

        # 1) ã€Œç·å–æ‰±å‡¦æ–¹ç®‹æ•°ã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æœ€å„ªå…ˆã§æ¢ã™
        for field_key, field_val in fields.items():
            if "ç·å–æ‰±å‡¦æ–¹ç®‹æ•°" in field_key:
                nums = re.findall(r"[\d,]+", field_val)
                if nums:
                    try:
                        n = int(nums[0].replace(",", ""))
                        if n > 0:
                            rx_annual = n
                            rx_period = "å¹´é–“å®Ÿç¸¾ï¼ˆå ±å‘ŠæœŸæ—¥ã®å‰å¹´1å¹´é–“ã®å–æ‰±å‡¦æ–¹ç®‹æšæ•°ï¼‰"
                            rx_raw = n
                            break
                    except (ValueError, OverflowError):
                        pass

        # 2) ä»–ã®å‡¦æ–¹ç®‹ãƒ»å—ä»˜é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if rx_annual is None:
            for field_key, field_val in fields.items():
                if not any(kw in field_key for kw in ["å‡¦æ–¹", "å—ä»˜å›æ•°"]):
                    continue
                # å›æ•°ãƒ»ä»¶æ•°ãƒ»æšæ•°ãªã©æ•°å€¤ã‚’å«ã‚€ã‚‚ã®
                nums = re.findall(r"[\d,]+", field_val)
                if not nums:
                    continue
                try:
                    n = int(nums[0].replace(",", ""))
                    if n == 0:
                        continue
                    if "é€±" in field_key or "é€±" in field_val:
                        rx_annual = int(n * 52.14)
                        rx_period = f"é€±å¹³å‡ {n}å› â†’ å¹´æ›ç®—ï¼ˆÃ— 52.14é€±ï¼‰"
                        rx_raw = n
                    elif "æœˆ" in field_key or "æœˆ" in field_val:
                        rx_annual = int(n * 12)
                        rx_period = f"æœˆå¹³å‡ {n}å› â†’ å¹´æ›ç®—ï¼ˆÃ— 12ãƒ¶æœˆï¼‰"
                        rx_raw = n
                    elif "å¹´" in field_key or "å¹´é–“" in field_val:
                        rx_annual = n
                        rx_period = "å¹´é–“å®Ÿç¸¾"
                        rx_raw = n
                    elif "æ—¥" in field_key:
                        rx_annual = int(n * NATIONAL_STATS["working_days"])
                        rx_period = f"1æ—¥å¹³å‡ {n}æš â†’ å¹´æ›ç®—ï¼ˆÃ— {NATIONAL_STATS['working_days']}æ—¥ï¼‰"
                        rx_raw = n
                    if rx_annual:
                        break
                except (ValueError, OverflowError):
                    continue

        # 3) ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ­£è¦è¡¨ç¾ã§æ¢ã™ï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if rx_annual is None:
            text_patterns = [
                (r"ç·å–æ‰±å‡¦æ–¹ç®‹æ•°[^\d]*(\d{1,3}(?:,\d{3})*|\d{4,})\s*ä»¶", "annual"),
                (r"é€±\s*å¹³å‡[^\d]{0,15}(\d{1,4}(?:,\d{3})*)\s*(?:å›|æš)", "weekly"),
                (r"æœˆ\s*å¹³å‡[^\d]{0,15}(\d{1,5}(?:,\d{3})*)\s*(?:å›|æš)", "monthly"),
                (r"å¹´é–“[^\d]{0,15}(\d{1,6}(?:,\d{3})*)\s*(?:å›|ä»¶|æš)", "annual"),
                (r"1æ—¥\s*(?:å¹³å‡)?[^\d]{0,15}(\d{2,3}(?:,\d{3})*)\s*(?:å›|æš)", "daily"),
            ]
            for pat, period in text_patterns:
                m = re.search(pat, full_text, re.DOTALL)
                if m:
                    try:
                        n = int(m.group(1).replace(",", ""))
                        if n == 0:
                            continue
                        if period == "weekly":
                            rx_annual = int(n * 52.14)
                            rx_period = f"é€±å¹³å‡ {n}å› â†’ å¹´æ›ç®—"
                        elif period == "monthly":
                            rx_annual = int(n * 12)
                            rx_period = f"æœˆå¹³å‡ {n}å› â†’ å¹´æ›ç®—"
                        elif period == "annual":
                            rx_annual = n
                            rx_period = "å¹´é–“å®Ÿç¸¾"
                        elif period == "daily":
                            rx_annual = int(n * NATIONAL_STATS["working_days"])
                            rx_period = f"1æ—¥å¹³å‡ {n}æš â†’ å¹´æ›ç®—"
                        rx_raw = n
                        break
                    except (ValueError, OverflowError):
                        continue

        data["prescriptions_annual"] = rx_annual
        data["prescription_period_label"] = rx_period
        data["prescription_raw_value"] = rx_raw

        return data


# ---------------------------------------------------------------------------
# 2. ã‚¦ã‚§ãƒ–æ¤œç´¢ï¼ˆDuckDuckGoï¼‰
# ---------------------------------------------------------------------------

class WebSearcher:
    DDG_URL = "https://html.duckduckgo.com/html/"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 Chrome/121.0.0.0 Safari/537.36"
            ),
        })

    def search(self, pharmacy_name: str) -> Tuple[Optional[Dict], str]:
        queries = [
            f'"{pharmacy_name}" å‡¦æ–¹ç®‹å—ä»˜å›æ•° è–¬å±€æ©Ÿèƒ½æƒ…å ±',
            f'{pharmacy_name} å‡¦æ–¹ç®‹ å¹´é–“ æšæ•° å—ä»˜',
        ]
        for query in queries:
            result = self._run(query)
            if result:
                return result, "ã‚¦ã‚§ãƒ–æ¤œç´¢ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹"
            time.sleep(0.5)
        return None, "ã‚¦ã‚§ãƒ–æ¤œç´¢ã§ã‚‚è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—"

    def _run(self, query: str) -> Optional[Dict]:
        try:
            r = self.session.post(
                self.DDG_URL,
                data={"q": query, "kl": "jp-jp"},
                timeout=12,
            )
            if r.status_code != 200:
                return None
            soup = BeautifulSoup(r.text, "html.parser")
            for result in soup.find_all("div", class_="result")[:5]:
                snippet_tag = result.find("a", class_="result__snippet") or result.find(
                    "div", class_="result__snippet"
                )
                link_tag = result.find("a", class_=re.compile(r"result.*url|result.*title", re.I))
                snippet = snippet_tag.get_text(strip=True) if snippet_tag else ""
                href = link_tag.get("href", "") if link_tag else ""
                extracted = self._extract(snippet)
                if extracted:
                    extracted["web_url"] = href
                    extracted["snippet"] = snippet
                    return extracted
        except Exception:
            pass
        return None

    def _extract(self, text: str) -> Optional[Dict]:
        for pat, period, mult in [
            (r"(\d{1,3}(?:,\d{3})*)\s*æš.*?å¹´", "annual", 1.0),
            (r"å¹´.*?(\d{1,3}(?:,\d{3})*)\s*æš", "annual", 1.0),
            (r"1æ—¥.*?(\d{2,3})\s*æš", "daily", 305.0),
            (r"é€±.*?(\d{2,4})\s*æš", "weekly", 52.14),
        ]:
            m = re.search(pat, text)
            if m:
                try:
                    val = float(m.group(1).replace(",", ""))
                    annual = int(val * mult)
                    if 500 <= annual <= 5_000_000:
                        return {"prescriptions_per_year": annual, "period": period}
                except (ValueError, OverflowError):
                    pass
        return None


# ---------------------------------------------------------------------------
# 3. çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è¨ˆ
# ---------------------------------------------------------------------------

class PharmacyEstimator:
    HOSPITAL_LARGE = ["å¤§å­¦ç—…é™¢", "åŒ»ç™‚ã‚»ãƒ³ã‚¿ãƒ¼", "å›½ç«‹ç—…é™¢", "å¸‚ç«‹ç—…é™¢",
                      "çœŒç«‹ç—…é™¢", "æ¸ˆç”Ÿä¼š", "æ—¥èµ¤", "èµ¤åå­—", "JCHO", "ãŒã‚“ã‚»ãƒ³ã‚¿ãƒ¼", "ä¸­å¤®ç—…é™¢"]
    HOSPITAL_GATE  = ["é–€å‰", "ç—…é™¢å‰", "ãƒ›ã‚¹ãƒ”ã‚¿ãƒ«"]
    HOSPITAL_NAME  = ["ç—…é™¢", "Hospital"]
    CLINIC_KW      = ["ã‚¯ãƒªãƒ‹ãƒƒã‚¯", "è¨ºç™‚æ‰€", "åŒ»é™¢", "å†…ç§‘", "å¤–ç§‘", "çš®è†šç§‘",
                      "æ•´å½¢å¤–ç§‘", "çœ¼ç§‘", "è€³é¼»ç§‘", "å°å…ç§‘", "ç”£å©¦äººç§‘"]

    def estimate(self, pharmacy_name: str, prefecture: str = "") -> SearchResult:
        refs: List[Dict] = []

        # ãƒã‚§ãƒ¼ãƒ³åˆ¤å®š
        matched_chain: Optional[str] = None
        chain_info: Optional[Dict] = None
        for chain, info in CHAIN_DATA.items():
            if chain in pharmacy_name:
                matched_chain = chain
                chain_info = info
                break

        # ç«‹åœ°åˆ¤å®š
        is_large_hosp = any(kw in pharmacy_name for kw in self.HOSPITAL_LARGE)
        is_hosp_gate  = is_large_hosp or any(kw in pharmacy_name for kw in self.HOSPITAL_GATE + self.HOSPITAL_NAME)
        is_clinic      = any(kw in pharmacy_name for kw in self.CLINIC_KW)

        # æ¨è¨ˆå€¤æ±ºå®š
        if matched_chain and is_large_hosp:
            pharmacy_type = f"å¤§ç—…é™¢é–€å‰ãƒã‚§ãƒ¼ãƒ³è–¬å±€ï¼ˆ{matched_chain}ï¼‰"
            annual_est = int(chain_info["annual_est"] * 2.5)
            min_val, max_val = int(chain_info["annual_est"] * 1.2), int(chain_info["annual_est"] * 5.0)
            confidence = "medium"
            basis = f"ã€Œ{matched_chain}ã€IRä»£è¡¨å€¤ {chain_info['annual_est']:,}æš/å¹´ Ã— å¤§ç—…é™¢é–€å‰ä¿‚æ•° 2.5"
            refs.append({"name": chain_info["ir"], "desc": "IRå…¬é–‹ãƒ‡ãƒ¼ã‚¿ Ã— ç—…é™¢é–€å‰ä¿‚æ•°ã‚ˆã‚Šæ¨è¨ˆ", "url": ""})

        elif matched_chain and is_hosp_gate:
            pharmacy_type = f"ç—…é™¢é–€å‰ãƒã‚§ãƒ¼ãƒ³è–¬å±€ï¼ˆ{matched_chain}ï¼‰"
            annual_est = int(chain_info["annual_est"] * 1.8)
            min_val, max_val = int(chain_info["annual_est"] * 0.9), int(chain_info["annual_est"] * 3.5)
            confidence = "medium"
            basis = f"ã€Œ{matched_chain}ã€IRä»£è¡¨å€¤ {chain_info['annual_est']:,}æš/å¹´ Ã— ç—…é™¢é–€å‰ä¿‚æ•° 1.8"
            refs.append({"name": chain_info["ir"], "desc": "IRå…¬é–‹ãƒ‡ãƒ¼ã‚¿ Ã— ç—…é™¢é–€å‰ä¿‚æ•°ã‚ˆã‚Šæ¨è¨ˆ", "url": ""})

        elif matched_chain and is_clinic:
            pharmacy_type = f"ã‚¯ãƒªãƒ‹ãƒƒã‚¯å‘¨è¾ºãƒã‚§ãƒ¼ãƒ³è–¬å±€ï¼ˆ{matched_chain}ï¼‰"
            annual_est = int(chain_info["annual_est"] * 1.1)
            min_val, max_val = int(chain_info["annual_est"] * 0.5), int(chain_info["annual_est"] * 2.0)
            confidence = "medium"
            basis = f"ã€Œ{matched_chain}ã€IRä»£è¡¨å€¤ {chain_info['annual_est']:,}æš/å¹´ Ã— ã‚¯ãƒªãƒ‹ãƒƒã‚¯ä¿‚æ•° 1.1"
            refs.append({"name": chain_info["ir"], "desc": "IRå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šæ¨è¨ˆ", "url": ""})

        elif matched_chain:
            pharmacy_type = f"ãƒã‚§ãƒ¼ãƒ³è–¬å±€ï¼ˆ{matched_chain}ï¼‰"
            annual_est = chain_info["annual_est"]
            min_val, max_val = chain_info["min"], chain_info["max"]
            confidence = "medium"
            basis = f"ã€Œ{matched_chain}ã€å…¬é–‹IRãƒ»è–¬å±€æ©Ÿèƒ½æƒ…å ±é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§"
            refs.append({"name": chain_info["ir"], "desc": "IRæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãƒ»è–¬å±€æ©Ÿèƒ½æƒ…å ±ã‚ˆã‚Š", "url": ""})

        elif is_large_hosp:
            pharmacy_type = "å¤§ç—…é™¢é–€å‰è–¬å±€ï¼ˆç‹¬ç«‹ç³»ï¼‰"
            annual_est, min_val, max_val = 80_000, 30_000, 200_000
            confidence = "low"
            basis = "å¤§å­¦ç—…é™¢ãƒ»åŒ»ç™‚ã‚»ãƒ³ã‚¿ãƒ¼ç­‰ã®å¤§ç—…é™¢é–€å‰ã¨ã—ã¦åˆ†é¡"

        elif is_hosp_gate:
            pharmacy_type = "ç—…é™¢é–€å‰è–¬å±€ï¼ˆç‹¬ç«‹ç³»ï¼‰"
            annual_est, min_val, max_val = 35_000, 12_000, 120_000
            confidence = "low"
            basis = "ç—…é™¢é–€å‰è–¬å±€ã¨ã—ã¦åˆ†é¡ï¼ˆåç§°ã«ç—…é™¢é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ï¼‰"

        elif is_clinic:
            pharmacy_type = "ã‚¯ãƒªãƒ‹ãƒƒã‚¯å‘¨è¾ºè–¬å±€"
            annual_est, min_val, max_val = 12_000, 4_000, 28_000
            confidence = "low"
            basis = "ã‚¯ãƒªãƒ‹ãƒƒã‚¯ãƒ»è¨ºç™‚æ‰€å‘¨è¾ºè–¬å±€ã¨ã—ã¦åˆ†é¡"

        else:
            pharmacy_type = "åœ°åŸŸå¯†ç€å‹è–¬å±€"
            annual_est, min_val, max_val = NATIONAL_STATS["median_estimate"], 2_000, 18_000
            confidence = "low"
            basis = f"å…¨å›½è–¬å±€ä¸­å¤®å€¤æ¨è¨ˆï¼ˆå³æ­ªã¿åˆ†å¸ƒã®ãŸã‚å¹³å‡{NATIONAL_STATS['average_per_year']:,}æšã‚ˆã‚Šä¸­å¤®å€¤ã‚’æ¡ç”¨ï¼‰"

        refs += [
            {
                "name": "åšç”ŸåŠ´åƒçœã€Œèª¿å‰¤åŒ»ç™‚è²»ï¼ˆé›»ç®—å‡¦ç†åˆ†ï¼‰ã®å‹•å‘ã€2022å¹´åº¦",
                "desc": (
                    f"å…¨å›½è–¬å±€æ•° {NATIONAL_STATS['total_pharmacies']:,}æ–½è¨­ / "
                    f"å¹´é–“å‡¦æ–¹ç®‹ {NATIONAL_STATS['total_prescriptions']//100_000_000:.1f}å„„æš / "
                    f"1æ–½è¨­å¹³å‡ {NATIONAL_STATS['average_per_year']:,}æš/å¹´"
                ),
                "url": NATIONAL_STATS["source_url"],
            },
            {
                "name": "æ—¥æœ¬è–¬å‰¤å¸«ä¼šã€Œè–¬å±€ãƒ»è–¬å‰¤å¸«ã«é–¢ã™ã‚‹åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã€",
                "desc": f"1æ—¥å¹³å‡å‡¦æ–¹ç®‹å—ä»˜æšæ•°: {NATIONAL_STATS['daily_average']}æšï¼ˆ2020å¹´èª¿æŸ»ï¼‰",
                "url": "https://www.nichiyaku.or.jp/",
            },
            {
                "name": "åšç”ŸåŠ´åƒçœ è–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦ï¼ˆåŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆ ãƒŠãƒ“ã‚¤ï¼‰",
                "desc": "å€‹åˆ¥è–¬å±€ã®æ©Ÿèƒ½æƒ…å ±ï¼ˆå‡¦æ–¹ç®‹å—ä»˜å›æ•°å«ã‚€ï¼‰ã€‚æœ¬ã‚¢ãƒ—ãƒªã§ç›´æ¥æ¤œç´¢å¯èƒ½ã€‚",
                "url": "https://www.iryou.teikyouseido.mhlw.go.jp/znk-web/juminkanja/S2300/initialize",
            },
        ]

        daily_est = annual_est // NATIONAL_STATS["working_days"]
        base_calc = NATIONAL_STATS["daily_average"] * NATIONAL_STATS["working_days"]

        methodology = [
            "### æ¨è¨ˆæ‰‹é †",
            "",
            "**STEP 1 â€” åšç”ŸåŠ´åƒçœãƒ‡ãƒ¼ã‚¿æ¤œç´¢**",
            "ã€€MHLWãƒãƒ¼ã‚¿ãƒ«ï¼ˆè–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦ï¼‰ã‚’æ¤œç´¢ã—ã¾ã—ãŸãŒã€",
            "ã€€å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆã—ã¾ã™ã€‚",
            "",
            "**STEP 2 â€” ãƒã‚§ãƒ¼ãƒ³è–¬å±€ãƒ»ç«‹åœ°ã‚¿ã‚¤ãƒ—åˆ¤å®š**",
            f"ã€€â†’ ã‚¿ã‚¤ãƒ—: **{pharmacy_type}**",
            f"ã€€â†’ æ ¹æ‹ : {basis}",
            "",
            "**STEP 3 â€” æ¨è¨ˆå€¤ç®—å‡º**",
            f"ã€€â†’ ä»£è¡¨å€¤: **{annual_est:,}æš/å¹´**",
            f"ã€€â†’ ãƒ¬ãƒ³ã‚¸: {min_val:,}ã€œ{max_val:,}æš/å¹´",
            f"ã€€â†’ 1æ—¥æ›ç®—: ç´„{daily_est}æš/æ—¥",
            "",
            "**STEP 4 â€” å…¨å›½çµ±è¨ˆã¨ã®æ•´åˆæ€§ç¢ºèª**",
            f"ã€€å…¨å›½å¹³å‡: æ—¥æ¬¡å¹³å‡{NATIONAL_STATS['daily_average']}æš Ã— {NATIONAL_STATS['working_days']}æ—¥ = {base_calc:,}æš/å¹´",
            "",
            "**âš  æ³¨æ„**: æœ¬æ¨è¨ˆã¯å‚è€ƒå€¤ã§ã™ã€‚æ­£ç¢ºãªæ•°å€¤ã¯åšåŠ´çœãƒãƒ¼ã‚¿ãƒ«ã§è–¬å±€åã‚’æ¤œç´¢ã™ã‚‹ã‹ã€",
            "å„è–¬å±€ã«ç›´æ¥ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
        ]

        return SearchResult(
            pharmacy_name=pharmacy_name,
            annual_prescriptions=annual_est,
            prescriptions_range=(min_val, max_val),
            daily_estimate=daily_est,
            data_source="statistical_estimation",
            source_label="çµ±è¨ˆãƒ¢ãƒ‡ãƒ«æ¨è¨ˆ",
            confidence=confidence,
            pharmacy_type=pharmacy_type,
            methodology=methodology,
            references=refs,
            mhlw_found=False,
            mhlw_has_rx_data=False,
        )


# ---------------------------------------------------------------------------
# Streamlit UI ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ---------------------------------------------------------------------------

def confidence_label(c: str) -> str:
    return {"high": "ğŸŸ¢ é«˜ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰", "medium": "ğŸŸ¡ ä¸­ï¼ˆIRãƒ»å…¬é–‹æƒ…å ±ï¼‰", "low": "ğŸ”´ ä½ï¼ˆçµ±è¨ˆæ¨è¨ˆï¼‰"}.get(c, "ä¸æ˜")


def render_result(result: SearchResult) -> None:
    # â”€â”€ ã‚½ãƒ¼ã‚¹ç¨®åˆ¥ãƒãƒŠãƒ¼
    if result.mhlw_found and result.mhlw_has_rx_data:
        st.success("âœ… **åšç”ŸåŠ´åƒçœ è–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦** ã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
    elif result.mhlw_found and not result.mhlw_has_rx_data:
        st.info(
            "â„¹ï¸ **åšç”ŸåŠ´åƒçœãƒãƒ¼ã‚¿ãƒ«** ã§è–¬å±€ã¯è¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã®è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            "çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§è£œå®Œã—ã¦ã„ã¾ã™ã€‚"
        )
    elif result.web_search_found:
        st.info("ğŸŒ **ã‚¦ã‚§ãƒ–æ¤œç´¢** ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹ã—ã¾ã—ãŸï¼ˆå‚è€ƒå€¤ï¼‰")
    else:
        st.warning(
            "ğŸ“Š åšåŠ´çœãƒãƒ¼ã‚¿ãƒ«ãƒ»ã‚¦ã‚§ãƒ–æ¤œç´¢ã§ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€"
            "**çµ±è¨ˆãƒ¢ãƒ‡ãƒ«**ã«ã‚ˆã‚‹æ¨è¨ˆå€¤ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™"
        )

    st.markdown("---")

    # â”€â”€ KPIã‚«ãƒ¼ãƒ‰
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("å¹´é–“å‡¦æ–¹ç®‹å—ä»˜æšæ•°", f"{result.annual_prescriptions:,} æš" if result.annual_prescriptions else "---")
    if result.prescriptions_range:
        c2.metric("æ¨è¨ˆãƒ¬ãƒ³ã‚¸ï¼ˆä¸‹é™ã€œä¸Šé™ï¼‰", f"{result.prescriptions_range[0]:,}ã€œ{result.prescriptions_range[1]:,}")
    c3.metric("1æ—¥ã‚ãŸã‚Šæ¨è¨ˆ", f"ç´„ {result.daily_estimate} æš/æ—¥" if result.daily_estimate else "---")
    c4.metric("ä¿¡é ¼åº¦", confidence_label(result.confidence))

    if result.pharmacy_type:
        st.caption(f"è–¬å±€ã‚¿ã‚¤ãƒ—: **{result.pharmacy_type}**")

    st.markdown("---")

    # â”€â”€ MHLW ã‹ã‚‰å–å¾—ã§ããŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¡¨ç¤º
    if result.mhlw_found and result.mhlw_fields:
        with st.expander("ğŸ“‹ åšç”ŸåŠ´åƒçœãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰"):
            display_fields = {k: v for k, v in result.mhlw_fields.items() if v and not k.startswith("field_")}
            if display_fields:
                for k, v in list(display_fields.items())[:50]:
                    st.text(f"  {k}: {v}")
            else:
                for k, v in list(result.mhlw_fields.items())[:50]:
                    st.text(f"  {k}: {v}")

    # â”€â”€ ã‚¿ãƒ–
    tab1, tab2, tab3 = st.tabs(["ğŸ“‹ æ¨è¨ˆãƒ­ã‚¸ãƒƒã‚¯", "ğŸ“š å‚ç…§ã‚½ãƒ¼ã‚¹", "ğŸ” æ¤œç´¢ãƒ­ã‚°"])

    with tab1:
        for line in result.methodology:
            st.markdown(line)

    with tab2:
        for i, ref in enumerate(result.references, 1):
            with st.expander(f"{i}. {ref['name']}"):
                st.write(ref.get("desc", ""))
                if ref.get("url"):
                    st.markdown(f"ğŸ”— [{ref['url']}]({ref['url']})")
        st.markdown("---")
        st.markdown(
            "#### ç›´æ¥ç¢ºèªã™ã‚‹\n"
            "- [åšç”ŸåŠ´åƒçœ åŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆï¼ˆãƒŠãƒ“ã‚¤ï¼‰è–¬å±€æ¤œç´¢]"
            "(https://www.iryou.teikyouseido.mhlw.go.jp/znk-web/juminkanja/S2300/initialize)"
            " â€” è–¬å±€åã§æ¤œç´¢ã—ã¦å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã‚’ç›´æ¥ç¢ºèªã§ãã¾ã™\n"
            "- [åšç”ŸåŠ´åƒçœ èª¿å‰¤åŒ»ç™‚è²»ã®å‹•å‘](https://www.mhlw.go.jp/topics/medias/med/)"
            " â€” å…¨å›½é›†è¨ˆçµ±è¨ˆ"
        )

    with tab3:
        if result.search_log:
            st.code("\n".join(result.search_log))
        else:
            st.write("ãƒ­ã‚°ãªã—")


# ---------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³
# ---------------------------------------------------------------------------

def main() -> None:
    st.set_page_config(
        page_title="è–¬å±€ å‡¦æ–¹ç®‹æšæ•°äºˆæ¸¬ãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ’Š",
        layout="wide",
        initial_sidebar_state="collapsed",
    )

    st.title("ğŸ’Š è–¬å±€ å¹´é–“å‡¦æ–¹ç®‹æšæ•° äºˆæ¸¬ãƒ„ãƒ¼ãƒ«")
    st.markdown(
        "è–¬å±€åã‚’å…¥åŠ›ã—ã¦**å€™è£œã‚’æ¤œç´¢**â†’å€™è£œã‹ã‚‰é¸æŠã™ã‚‹ã¨ã€"
        "åšç”ŸåŠ´åƒçœãƒ‡ãƒ¼ã‚¿ï¼ˆåŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆ ãƒŠãƒ“ã‚¤ï¼‰ã‹ã‚‰å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚"
        "ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆã—ã¾ã™ã€‚"
    )

    # â”€â”€ session_state åˆæœŸåŒ–
    for key, default in [
        ("candidates", []),
        ("total_count", 0),
        ("selected_idx", 0),
        ("final_result", None),
        ("search_done", False),
    ]:
        if key not in st.session_state:
            st.session_state[key] = default

    # ================================================================
    # STEP 1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ› + å€™è£œæ¤œç´¢
    # ================================================================
    st.markdown("### STEP 1 â€” è–¬å±€åã‚’å…¥åŠ›ã—ã¦å€™è£œã‚’æ¤œç´¢")

    col_kw, col_pref = st.columns([3, 1])
    with col_kw:
        keyword = st.text_input(
            "è–¬å±€åï¼ˆä¸€éƒ¨ã§ã‚‚å¯ï¼‰",
            placeholder="ä¾‹: ã‚¦ã‚¨ãƒ«ã‚·ã‚¢ æ¸‹è°· / æ—¥æœ¬èª¿å‰¤ æ–°å®¿ / ã¾ã¤ã‚‚ã¨è–¬å±€",
            help="å®Œå…¨ä¸€è‡´ã§ãªãã¦ã‚‚OKã§ã™ã€‚éƒ¨åˆ†æ–‡å­—åˆ—ã§æ¤œç´¢ã—ã¾ã™ã€‚",
            key="keyword_input",
        )
    with col_pref:
        prefecture = st.selectbox("éƒ½é“åºœçœŒï¼ˆä»»æ„ï¼‰", ["ï¼ˆæŒ‡å®šãªã—ï¼‰"] + PREFECTURES, key="pref_select")

    col_btn1, col_opt = st.columns([2, 1])
    with col_btn1:
        search_btn = st.button("ğŸ” å€™è£œã‚’æ¤œç´¢ï¼ˆMHLWãƒãƒ¼ã‚¿ãƒ«ï¼‰", use_container_width=True, type="primary")
    with col_opt:
        skip_mhlw = st.checkbox("MHLWãƒãƒ¼ã‚¿ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆçµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰", value=False)

    if search_btn and keyword.strip():
        pref_code = PREFECTURE_CODES.get(prefecture, "")
        st.session_state["final_result"] = None
        st.session_state["search_done"] = False

        if skip_mhlw:
            # çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã®ã¿
            estimator = PharmacyEstimator()
            result = estimator.estimate(keyword.strip(), prefecture)
            result.search_log = ["[MHLW] ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šï¼‰"]
            st.session_state["final_result"] = result
            st.session_state["search_done"] = True
            st.session_state["candidates"] = []
        else:
            with st.spinner("åšç”ŸåŠ´åƒçœãƒãƒ¼ã‚¿ãƒ«ã‚’æ¤œç´¢ä¸­â€¦ï¼ˆåˆå›ã¯10ã€œ20ç§’ã‹ã‹ã‚Šã¾ã™ï¼‰"):
                scraper = MHLWScraper()
                candidates, total, status_msg = scraper.search_candidates(
                    keyword.strip(), pref_code, max_pages=3
                )

            st.session_state["candidates"] = candidates
            st.session_state["total_count"] = total
            st.session_state["selected_idx"] = 0

            if not candidates:
                st.warning(f"MHLWãƒãƒ¼ã‚¿ãƒ«ã§å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆ{status_msg}ï¼‰ã€‚çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆã—ã¾ã™ã€‚")
                estimator = PharmacyEstimator()
                result = estimator.estimate(keyword.strip(), prefecture)
                result.search_log = [f"[MHLW] {status_msg}", "[ESTIMATE] çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è¨ˆã‚’å®Ÿè¡Œ"]
                st.session_state["final_result"] = result
                st.session_state["search_done"] = True
            else:
                st.success(f"âœ… {status_msg}ï¼ˆå…¨{total}ä»¶ä¸­ æœ€å¤§60ä»¶è¡¨ç¤ºï¼‰")

    # ================================================================
    # STEP 2: å€™è£œé¸æŠ + è©³ç´°å–å¾—
    # ================================================================
    candidates: List[PharmacyCandidate] = st.session_state.get("candidates", [])

    if candidates and st.session_state.get("final_result") is None:
        st.markdown("---")
        st.markdown("### STEP 2 â€” è–¬å±€ã‚’é¸æŠã—ã¦è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

        # ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³é¸æŠè‚¢ã‚’ä½œæˆï¼ˆåå‰ + ä½æ‰€ï¼‰
        options = [
            f"{c.name}ã€€{('ï¼ˆ' + c.address[:35] + 'ï¼‰') if c.address else ''}"
            for c in candidates
        ]

        selected_label = st.selectbox(
            f"å€™è£œä¸€è¦§ï¼ˆ{len(candidates)}ä»¶ï¼‰",
            options,
            index=st.session_state["selected_idx"],
            key="candidate_select",
        )
        sel_idx = options.index(selected_label)
        st.session_state["selected_idx"] = sel_idx
        sel_candidate = candidates[sel_idx]

        # é¸æŠã•ã‚ŒãŸè–¬å±€ã®è©³ç´°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        col_info1, col_info2 = st.columns(2)
        col_info1.caption(f"ğŸ“ ä½æ‰€: {sel_candidate.address or 'ä¸æ˜'}")
        col_info2.caption(
            f"ğŸ”— MHLWãƒšãƒ¼ã‚¸: [è©³ç´°ã‚’è¦‹ã‚‹]({sel_candidate.href})" if sel_candidate.href else ""
        )

        col_fetch, col_stat = st.columns([2, 1])
        with col_fetch:
            fetch_btn = st.button(
                "ğŸ“„ ã“ã®è–¬å±€ã®å‡¦æ–¹ç®‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—", use_container_width=True, type="primary"
            )
        with col_stat:
            use_stat = st.button("ğŸ“Š çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆ", use_container_width=True)

        if fetch_btn:
            with st.spinner(f"ã€Œ{sel_candidate.name}ã€ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­â€¦"):
                scraper = MHLWScraper()
                ok, _ = scraper.initialize_session()
                detail, detail_msg = scraper.get_pharmacy_detail(sel_candidate)

            log = [
                f"[MHLW] å€™è£œé¸æŠ: {sel_candidate.name}",
                f"[MHLW] è©³ç´°å–å¾—: {detail_msg}",
            ]

            if detail and detail.get("prescriptions_annual"):
                rx = detail["prescriptions_annual"]
                daily = rx // NATIONAL_STATS["working_days"]
                period_label = detail.get("prescription_period_label", "ä¸æ˜")
                result = SearchResult(
                    pharmacy_name=sel_candidate.name,
                    annual_prescriptions=rx,
                    daily_estimate=daily,
                    data_source="mhlw_portal",
                    source_label="åšç”ŸåŠ´åƒçœ è–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦",
                    source_url=detail.get("source_url", sel_candidate.href),
                    confidence="high",
                    pharmacy_type="MHLWãƒãƒ¼ã‚¿ãƒ«å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿",
                    methodology=[
                        "### å–å¾—æ–¹æ³•",
                        "",
                        "**åšç”ŸåŠ´åƒçœ åŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆï¼ˆãƒŠãƒ“ã‚¤ï¼‰è–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦** ã‚ˆã‚Šç›´æ¥å–å¾—ã€‚",
                        "",
                        f"**å–å¾—å€¤**: {period_label}",
                        f"**å¹´é–“æ›ç®—**: {rx:,}æš/å¹´",
                        f"**1æ—¥æ›ç®—**: ç´„{daily}æš/æ—¥ï¼ˆå¹´é–“ç¨¼åƒæ—¥æ•°{NATIONAL_STATS['working_days']}æ—¥ã§é™¤ç®—ï¼‰",
                        "",
                        f"**MHLWãƒãƒ¼ã‚¿ãƒ«URL**: {detail.get('source_url', '')}",
                    ],
                    references=[
                        {
                            "name": "åšç”ŸåŠ´åƒçœ è–¬å±€æ©Ÿèƒ½æƒ…å ±æä¾›åˆ¶åº¦",
                            "desc": "è–¬å±€ãŒæ¯å¹´éƒ½é“åºœçœŒã«å ±å‘Šã™ã‚‹æ©Ÿèƒ½æƒ…å ±ï¼ˆå‡¦æ–¹ç®‹å—ä»˜å›æ•°ã‚’å«ã‚€ï¼‰",
                            "url": detail.get("source_url", sel_candidate.href),
                        }
                    ],
                    mhlw_found=True,
                    mhlw_has_rx_data=True,
                    search_log=log,
                    mhlw_fields=detail.get("all_fields", {}),
                )
            elif detail:
                # MHLW ã§è–¬å±€ã¯è¦‹ã¤ã‹ã£ãŸãŒå‡¦æ–¹ç®‹æ•°ãƒ‡ãƒ¼ã‚¿ãªã— â†’ çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§è£œå®Œ
                log.append("[MHLW] å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã®è¨˜è¼‰ãªã— â†’ çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§è£œå®Œ")
                estimator = PharmacyEstimator()
                result = estimator.estimate(sel_candidate.name)
                result.mhlw_found = True
                result.mhlw_has_rx_data = False
                result.mhlw_fields = detail.get("all_fields", {})
                result.search_log = log
                result.source_url = detail.get("source_url", sel_candidate.href)
                result.methodology = [
                    "### å–å¾—æ–¹æ³•",
                    "",
                    "**åšç”ŸåŠ´åƒçœãƒãƒ¼ã‚¿ãƒ«** ã§è–¬å±€æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸãŒã€",
                    "å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã®é …ç›®ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚",
                    "ï¼ˆè–¬å±€ã«ã‚ˆã£ã¦ã¯å ±å‘Šã—ã¦ã„ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰",
                    "",
                ] + result.methodology
            else:
                # è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•— â†’ çµ±è¨ˆãƒ¢ãƒ‡ãƒ«
                log.append(f"[MHLW] è©³ç´°å–å¾—å¤±æ•—: {detail_msg} â†’ çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆ")
                estimator = PharmacyEstimator()
                result = estimator.estimate(sel_candidate.name)
                result.search_log = log

            st.session_state["final_result"] = result
            st.session_state["search_done"] = True

        if use_stat:
            estimator = PharmacyEstimator()
            result = estimator.estimate(sel_candidate.name)
            result.search_log = [f"[STAT] ã€Œ{sel_candidate.name}ã€ã‚’çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è¨ˆ"]
            st.session_state["final_result"] = result
            st.session_state["search_done"] = True

    # ================================================================
    # STEP 3: çµæœè¡¨ç¤º
    # ================================================================
    final_result: Optional[SearchResult] = st.session_state.get("final_result")

    if final_result:
        st.markdown("---")
        st.markdown(f"## çµæœ: `{final_result.pharmacy_name}`")
        render_result(final_result)

    # â”€â”€ åˆæœŸç”»é¢ï¼ˆæœªæ¤œç´¢æ™‚ï¼‰
    if not st.session_state.get("search_done") and not candidates and final_result is None:
        st.markdown("---")
        st.markdown("### å…¨å›½çµ±è¨ˆï¼ˆå‚è€ƒï¼‰")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("å…¨å›½è–¬å±€æ•°ï¼ˆ2022å¹´ï¼‰", f"{NATIONAL_STATS['total_pharmacies']:,} æ–½è¨­")
        c2.metric("å¹´é–“å‡¦æ–¹ç®‹ç·æ•°", f"{NATIONAL_STATS['total_prescriptions'] // 100_000_000:.1f} å„„æš")
        c3.metric("1æ–½è¨­ã‚ãŸã‚Šå¹³å‡", f"{NATIONAL_STATS['average_per_year']:,} æš/å¹´")
        c4.metric("1æ—¥å¹³å‡å—ä»˜æšæ•°", f"{NATIONAL_STATS['daily_average']} æš/æ—¥")
        st.caption(f"å‡ºå…¸: {NATIONAL_STATS['source']}")

        st.markdown("---")
        st.markdown("### ä½¿ã„æ–¹")
        col1, col2, col3 = st.columns(3)
        col1.markdown(
            "**â‘  è–¬å±€åã‚’å…¥åŠ›**\n\n"
            "å®Œå…¨ä¸€è‡´ã§ãªãã¦OKã§ã™ã€‚\n"
            "ã€Œã‚¦ã‚¨ãƒ«ã‚·ã‚¢ æ¸‹è°·ã€ã®ã‚ˆã†ã«\nãƒã‚§ãƒ¼ãƒ³åï¼‹åœ°åã§çµã‚Šè¾¼ã‚ã¾ã™ã€‚"
        )
        col2.markdown(
            "**â‘¡ å€™è£œã‹ã‚‰é¸æŠ**\n\n"
            "MHLWãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰æœ€å¤§60ä»¶ã®\n"
            "å€™è£œãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n"
            "ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ã§æ­£ã—ã„è–¬å±€ã‚’é¸æŠã€‚"
        )
        col3.markdown(
            "**â‘¢ ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ¨è¨ˆ**\n\n"
            "å‡¦æ–¹ç®‹å—ä»˜å›æ•°ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€‚\n"
            "è¨˜è¼‰ãŒãªã„å ´åˆã¯IRãƒ»çµ±è¨ˆãƒ‡ãƒ¼ã‚¿\nã‹ã‚‰è‡ªå‹•æ¨è¨ˆã—ã¾ã™ã€‚"
        )


if __name__ == "__main__":
    main()
